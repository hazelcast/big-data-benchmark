# Streaming Benchmark

The streaming benchmark is intended to measure the latency overhead
for a streaming system under different conditions such as message
rate and window size. It compares Hazelcast Jet, Apache Flink,
and Apache Spark Streaming.

## How to Run the Benchmarks

All benchmarks begin with these common steps:

1. Start Apache Kafka, see the [Kafka
   documentation](https://kafka.apache.org/documentation/)

2. Build the benchmarking code:

```bash
$ cd /path/to/big-data-benchmark
$ mvn clean package -pl trade-monitor/kafka-trade-producer -am
```

3. Start the program that produces events to Kafka:

```bash
$ cd trade-monitor/kafka-trade-producer
$ java -cp target/kafka-trade-producer-1.0-SNAPSHOT.jar \
com.hazelcast.jet.benchmark.trademonitor.KafkaTradeProducer \
localhost:9092 4 \
1_000_000 1024 OBJECT
```

The parameters are:

```
<Kafka broker URI> <num parallel producers> \
<trades per second> <num distinct keys> <messageType>
```

The producer will emit the given number of trade events per second to
Kafka topic "trades", and it will use `<num producers>` threads to do
it. Every thread runs its own instance of a Kafka Producer client and
produces its share of the requested events per second, using its
distinct share of the requested keyset size.

The trade event timestamps are predetermined and don't depend on
wall-clock time. Effectively, this program simulates a constant stream
of equally-spaced trade events. It guarantees it won't try to send an
event to Kafka before it has occurred, but there's no guarantee on how
much later it will manage to send it. If the requested throughput is too
high, the producer may be increasingly falling back behind real time.
You can track this in the program's output.

## Jet benchmark

1. Download and unzip the Hazelcast Jet distribution package, see the
[Jet documentation](https://jet-start.sh/docs/operations/installation).

2. Build the Pipeline JAR and copy to the `lib` folder on all Jet nodes.
Here we copy it to the local Jet installation:

```bash
$ cd /path/to/big-data-benchmark
$ mvn clean package -pl trade-monitor/jet-trade-monitor -am
$ cp trade-monitor/jet-trade-monitor/target/jet-trade-monitor-1.0-SNAPSHOT.jar \
/path/to/hazelcast-jet/lib
```

3. Start a Jet node:

```bash
$ cd /path/to/hazelcast-jet
$ bin/jet-start
```

3. Submit the job

```bash
$ cd /path/to/hazelcast-jet
$ bin/jet -v submit lib/jet-trade-monitor-1.0-SNAPSHOT.jar \
localhost:9092 latest OBJECT \
4 5000 \
1000 100 \
AT_LEAST_ONCE 10000 \
20 240 benchmark-results/
```

The parameters are:

```
<Kafka broker URI> <message offset auto-reset> <message type>
<Kafka source local parallelism> <max event lag ms> \
<window size ms> <sliding step ms> \
<processing guarantee> <snapshot interval ms> \
<warmup seconds> <measurement seconds> <output path>
```

The submitted job reads the trade events that the producer program sent
to Kafka. It performs sliding window aggregation on them (determines
trades-per-second for each ticker) and then records the end-to-end
latency: how much after the window's end timestamp was Jet able to emit
the first key-value pair of the window result.

Jet guarantees it won't emit a window result before having observed all
the events up to its end time. If you set the _allowed event lag_ to
more than zero, it will increase the end-to-end latency by that much
because it tells Jet how much to wait for any late-coming events. When
you use more than one producer, since they send their events
independently, this usually results in some event disorder.

Watch the console output at the server. When you see "benchmarking is
done" being repeatedly printed, you can stop the job with Ctrl-C.

4. Retrieve the Results

The results will be on the Jet server, in the directory you specified
when submitting the job. In our example it is the `benchmark-results`
folder. You should find two subfolders there:

- `latency-log` contains the raw log of all the recorded latencies
- `latency-profile` contains the latency profile generated by
  `HdrHistogram`

In both cases the filename is just "0", that's how Jet saves them.

To visualize a HdrHistogram, you can go to [this
page](https://hdrhistogram.github.io/HdrHistogram/plotFiles.html).

To visualize the raw latency log, you can use `gnuplot` :

```bash
$ cd /path/to/hazelcast-jet/benchmark-results/latency-log
$ gnuplot -e "set datafile separator ','; plot '0'; pause -1"
```
